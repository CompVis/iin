<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>

    <title>
      A Disentangling Invertible Interpretation Network for Explaining Latent
      Representations
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner">
        <h2>
          A Disentangling Invertible Interpretation Network for <br/>
          Explaining Latent Representations
        </h2>
        <p>
        <a href="https://github.com/pesser">Patrick Esser</a>&ast;, 
        <a href="https://github.com/rromb">Robin Rombach</a>&ast;,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Bj&ouml;rn Ommer</a><br/>
        <a href="https://www.iwr.uni-heidelberg.de/">IWR, Heidelberg University</a><br/>
        <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a></p>
			</section>

			<!-- One -->
				<section id="one" class="wrapper style1">
					<div class="container 75%">
						<div class="row 200%">
							<div class="6u 12u$(medium) vert-center" style="margin:1% 0">
                  <div class="container 25%">

                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="images/overview.jpg" alt="" style="border:0px solid black"/>
                      Our Invertible Interpretation Network <var>T</var> can be applied
                      to arbitrary existing models. Its invertibility
                      guarantees that the translation from <var>z</var> to
                      <var>z&#771;</var> does
                      not affect the performance of the model to be
                      interpreted.
                    </div>

                    <div class="image fit captioned align-center"
                                style="margin-bottom:4em; box-shadow:0 0">
                      <a href="https://arxiv.org/pdf/2004.13166.pdf">
                        <img src="images/paper.jpg" alt="" style="border:1px solid black"/>
                      </a>
                      <a href="https://arxiv.org/abs/2004.13166">arXiv</a>
                      <div class="headerDivider"></div>
                      <a href="images/iin.bib">BibTeX</a>
                      <div class="headerDivider"></div>
                      <a href="https://github.com/CompVis/iin">GitHub</a>
                      <br/>
                      &ast; equal contribution
                    </div>

                  </div>
							</div>
							<div class="6u$ 12u$(medium)">
                <h1>Abstract</h1>
                <p style="text-align: justify">
Neural networks have greatly boosted performance in computer vision by learning
powerful representations of input data. The drawback of end-to-end training for
maximal overall performance are black-box models whose hidden representations
are lacking interpretability: Since distributed coding is optimal for latent
layers to improve their robustness, attributing meaning to parts of a hidden
feature vector or to individual neurons is hindered.  We formulate
interpretation as a translation of hidden representations onto semantic
concepts that are comprehensible to the user. The mapping between both domains
has to be bijective so that semantic modifications in the target domain
correctly alter the original representation. The proposed invertible
interpretation network can be transparently applied on top of existing
architectures with no need to modify or retrain them.  Consequently, we
translate an original representation to an equivalent yet interpretable one and
backwards without affecting the expressiveness and performance of the original.
The invertible interpretation network disentangles the hidden representation
into separate, semantically meaningful concepts. Moreover, we present an
efficient approach to define semantic concepts by only sketching two images and
also an unsupervised strategy. Experimental evaluation demonstrates the wide
applicability to interpretation of existing classification and image generation
networks as well as to semantically guided image manipulation.
                </p>
							</div>
<!-- related works !-->

<div class="12u">
  <h4>New Works on Understanding and Disentangling Latent Representations with INNs</h4>
</div>

<div class="12u">
  <h6>
    <a href="https://compvis.github.io/net2net/">
      Network-to-Network Translation with Conditional Invertible Neural Networks
    </a>
  </h6>
</div>
<div class="3u 12u$(medium)">
  <div class="image fit align-center">
    <a href="https://compvis.github.io/net2net/">
      <img src="https://compvis.github.io/net2net/paper/teaser.png" style="max-width:25em; margin:auto" />
    </a>
  </div>
</div>
<div class="9u 12u$(medium)">
  <p align="justify" style="line-height: 1.0em; font-size:0.8em">
  Given the ever-increasing computational costs of modern machine learning models, we need to find new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. Therefore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse domains, (ii) enabling controlled content synthesis by allowing modification in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between fixed representations without having to learn or finetune them. This allows users to utilize various existing domain-specific expert models from the literature that had been trained with extensive computational resources. Experiments on diverse conditional image synthesis tasks, competitive image modification results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own.
  </p>
</div>

<div class="12u">
  <h6>
    <a href="https://compvis.github.io/invariances/">
      Making Sense of CNNs: Interpreting Deep Representations & Their Invariances with INNs (2020)
    </a>
  </h6>
</div>
<div class="3u 12u$(medium)">
  <div class="image fit align-center">
    <a href="https://compvis.github.io/invariances/">
      <img src="https://compvis.github.io/invariances/images/overview.jpg" style="max-width:25em; margin:auto" />
    </a>
  </div>
</div>
<div class="9u 12u$(medium)">
  <p align="justify" style="line-height: 1.0em; font-size:0.8em">
  To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-specific representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the different semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-specific, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach significantly extends the abilities to understand black box models by enabling post-hoc interpretations of state-of-the-art networks without compromising their performance.
  </p>
</div>


<!-- /related works !-->
						</div>
					</div>
				</section>

			<!-- Two -->
				<section id="two" class="wrapper style2 special">
					<div class="container">
						<header class="major">
							<h2>Results</h2>
							<p>and applications of our invertible interpretation network.</p>
						</header>

            <div class="row 150%">
<div class="6u 12u$(xsmall)">
								<div class="image fit captioned align-just">
                  <div class="videocontainer">
                  <video poster="images/celeba.jpg" controls class="videothing">
                   <source src="images/celeba.mp4" type="video/mp4">
                   Your browser does not support the video tag.
                  </video>
                  </div>
                  <h3>Semantic image modifications and embeddings on CelebA</h3>
                  We interpolate within individual semantic concepts and
                  visualize representations embedded onto semantically-meaningful
                  dimensions.
								</div>
</div>
<div class="6u$ 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <div class="videocontainer">
                  <video poster="images/mnist.jpg" controls class="videothing">
                   <source src="images/mnist.mp4" type="video/mp4">
                   Your browser does not support the video tag.
                  </video>
                  </div>
                  <h3>Semantic image modifications and embeddings on CMNIST</h3>
                  We interpolate within individual semantic concepts and
                  visualize representations embedded onto semantically-meaningful
                  dimensions.
								</div>
</div>
</div>
<div class="row 150%">
<div class="6u 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <a href="images/animalswap.jpg">
									<img src="images/animalswap.jpg" alt="" />
                  </a>
                  <h3>Transfer on AnimalFaces</h3>
                  We combine <var>z&#771;<sub>0</sub></var> (residual) of the
                  target image (leftmost column) with
                  <var>z&#771;<sub>1</sub></var> (animal class) of the source
                  image (top row), resulting in a transfer of animal type from source to
                  target.
								</div>
</div>
<div class="6u$ 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <a href="images/linear.jpg">
									<img src="images/linear.jpg" alt="" />
                  </a>
									<h3>Linearization of Latent Space</h3>
                  The inverse of our interpretation network <var>T</var> maps linear walks
                  in the interpretable domain back to nonlinear walks on the data manifold in
                  the encoder space, which get decoded to meaningful images (bottom right). In contrast,
                  decoded images of linear walks in the encoder space contain ghosting
                  artifacts (bottom left).
								</div>
</div>
</div>
<div class="row 150%">
<div class="6u 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <a href="images/fid.jpg">
									<img src="images/fid.jpg" alt="" />
                  </a>
									<h3>Turning Autoencoders into Generative Models</h3>
                  Applied to latent representations <var>z</var> of an autoencoder, our
                  approach enables semantic image analogies. After sampling an interpretable
                  representation <var>z&#771;</var>, we use the
                  inverse of <var>T</var> to transform it to
                  a latent representation <var>z=T<sup>-1</sup>(z&#771;)</var> of the
                  autoencoder and obtain a sampled image after decoding
                  <var>z</var>. Our approach
                  significantly improves FID scores compared to previous autoencoder based
                  generative models and simple GAN models.
								</div>
</div>
<div class="6u$ 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <a href="images/analogy.jpg">
									<img src="images/analogy.jpg" alt="" />
                  </a>
									<h3>Autoencoder Analogies</h3>
                  Applied to latent representations <var>z</var> of an autoencoder, our
                  approach enables semantic image analogies. After transforming
                  <var>z</var> to disentangled
                  semantic factors <var>(z&#771;<sub>k</sub>)=T(z)</var>, we
                  replace <var>z&#771;<sub>k</sub></var> of
                  the target image (leftmost column), with
                  <var>z&#771;<sub>k</sub></var> of the source image
                  (top row). From left to right: k=1 (digit), k=2 (color), k=0 (residual).
								</div>
</div>
</div>
<div class="row 150%">
<div class="6u 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <a href="images/walk.jpg">
									<img src="images/walk.jpg" alt="" />
                  </a>
									<h3>Semantic Manipulations of Representations</h3>
                  Modifications of latent representations <var>z</var> of a CMNIST
                  classifier visualized through UMAP embeddings.
                  Colors
                  of dots represent classes of test examples.  We map latent
                  representations <var>z</var> to
                  interpretable representations <var>z&#771;=T(z)</var>, where we perform a random walk in
                  one of the factors <var>z&#771;<sub>k</sub></var>. Using
                  <var>T<sup>-1</sup></var>, this random walk
                  is mapped back to the latent space and shown as black crosses connected by gray
                  lines.  On the left, a random walk in the digit factor jumps between digit
                  clusters, whereas on the right, a random walk in the color factor stays
                  (mostly) within the digit cluster it starts from.
								</div>
</div>
<div class="6u$ 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <a href="images/sensitivity.jpg">
									<img src="images/sensitivity.jpg" alt="" />
                  </a>
									<h3>Network Response Analysis</h3>
                  Left: Output variance per class of a digit classifier on
                  ColorMNIST, assessed via distribution of class
                  predictions. <var>T</var> disentangles <var>z&#771;<sub>0</sub></var>
                  (residual), <var>z&#771;<sub>1</sub></var>
                  (digit) and <var>z&#771;<sub>2</sub></var> (color).
                  The distribution of predicted classes is indeed not sensitive to
                  variations in the factor color, but turns out to be quite
                  responsive when altering the digit representation.
                  Right: 1d disentangled UMAP
                  embeddings of <var>z&#771;<sub>1</sub></var> and
                  <var>z&#771;<sub>2</sub></var>.
								</div>
</div>
</div>
<div class="row 150%">
<div class="6u 12u$(xsmall)">

								<div class="image fit captioned align-just">
                  <div class="videocontainer">
                  <video controls class="videothing">
                   <source src="images/5718-1min.mp4" type="video/mp4">
                   Your browser does not support the video tag.
                  </video>
                  </div>
                  <h3>Overview</h3>
                  Our presentation for the
                  <a href="http://cvpr20.com">virtual CVPR conference</a>.
								</div>
</div>
</div>


				  </div>
				</section>


			<!-- Four -->
				<section id="four" class="wrapper style3 special">
					<div class="container">
						<header class="major">
							<h2>Acknowledgement</h2>
              <p>
              This work has been supported in part by the German federal ministry BMWi within
              the project <q>KI Absicherung</q>, the German Research Foundation (DFG) projects
              371923335 and 421703927, and a hardware donation from NVIDIA corporation.
              This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
              </p>
						</header>
					</div>
				</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
